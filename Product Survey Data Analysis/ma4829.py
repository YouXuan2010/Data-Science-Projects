# -*- coding: utf-8 -*-
"""MA4829.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1sOsnlZuZSsKLF0UwtmAyNRo0w_OUIkAJ

# Product Survery Data Analysis

## Problem statement

1. Predict customer group based on customer information using MCA (Multiple correspondence analysis) Clustering.
2. Find correlation between different interior and exterior components
3. Conduct market basket analysis using association rule mining and find  relationship between interior and exterior components (Provide customization package for customer)

## Import datasets and libraries
"""

!wget -O SURVEY_RESULTS_2024.xlsx "https://docs.google.com/spreadsheets/d/e/2PACX-1vQuESCb9PI3v1XvsfCJd7_pfGnW4s6SnGJ0bT1Q8JS8GTZ5oT8xG-hdRW191vMMUw/pub?output=xlsx"

import pandas as pd
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import LabelEncoder

df = pd.read_excel('SURVEY_RESULTS_2024.xlsx')
df.head(5)

"""## Exploratory data analysis

"""

df.describe()

def perform_EDA(dataframe):
    print("Dataset Information ".center(50, '-'))
    print('')
    print(dataframe.info())
    print('')
    print("Duplicated Values ".center(50, '-'))
    print('')
    print('Sum of Duplicated Values:', dataframe.duplicated().sum())
    print('')
    print('')
    print("Unique Values ".center(50, '-'))
    print('')
    print(dataframe.nunique())
    print('')

perform_EDA(df)

def find_col_dtypes(data, ord_th):
    num_cols = data.select_dtypes("number").columns.tolist()
    cat_cols = data.select_dtypes("object").columns.tolist()

    ordinals = [col for col in num_cols if data[col].nunique() < ord_th]

    return num_cols, ordinals, cat_cols

num_cols, ordinals, cat_cols = find_col_dtypes(df, 20)

print(f"Num Cols: {len(num_cols)}\n")
print(f"Cat Cols: {len(cat_cols)}\n")
print(f"Ordinal Cols: {len(ordinals)}")

filtered_cat_cols = [col for col in cat_cols if df[col].nunique() <= 5]

filtered_cat_df = pd.DataFrame({'Categorical Columns': filtered_cat_cols})
pd.set_option('display.max_colwidth', None)

print(filtered_cat_df)

# Wrap title for better visualization
def wrap_title(title, max_length=55):
    if len(title) > max_length:
        break_point = title.rfind(' ', 0, max_length)
        if break_point == -1:
            break_point = max_length
        title = f'{title[:break_point]}\n{title[break_point+1:]}'
    return title

def cat_summary(dataframe, cat_cols, max_categories=5):
    filtered_cat_cols = [col for col in cat_cols if dataframe[col].nunique() <= max_categories] # Sort out "REAL" categorical variables
    num_cols = len(filtered_cat_cols)
    num_rows = (num_cols + 1) // 2

    if num_rows == 0:
        print("No categorical columns meet the criteria.")
        return

    fig, axes = plt.subplots(num_rows, 2, figsize=(15, 5 * num_rows), sharey=True, gridspec_kw={'hspace': 1.0})
    fig.patch.set_facecolor('#EBF5FB')
    flat_axes = axes.flatten()

    for i, col_name in enumerate(filtered_cat_cols):
        row_index = i // 2
        col_index = i % 2
        sns.countplot(x=dataframe[col_name], data=dataframe, color='#42328d', ax=flat_axes[i])

        labels = [wrap_title(label.get_text(), max_length=25) for label in flat_axes[i].get_xticklabels()]
        plt.sca(flat_axes[i])
        plt.xticks(flat_axes[i].get_xticks(), labels)

        wrapped_title = wrap_title(col_name)
        flat_axes[i].set_title(wrapped_title, fontsize=12)
        flat_axes[i].tick_params(axis='x')

    for i in range(num_cols, num_rows * 2):
        fig.delaxes(flat_axes[i])

    plt.tight_layout()
    plt.show()

cat_summary(df, cat_cols)

"""## Data cleaning / Feature engineering"""

df.describe()

# Fill missing column with None
def preprocess(df):

    for col in df.select_dtypes(exclude='number').columns:
        df[col] = df[col].fillna('None')

    for col in df.select_dtypes(include='number').columns:
        df[col] = df[col].fillna(0)

    return df

cleaned_df = preprocess(df)
cleaned_df.describe()

df = pd.read_excel('SURVEY_RESULTS_2024.xlsx')
cleaned_df = preprocess(df)
test = cleaned_df.copy()

def extract_attributes(column_data):
    all_attributes = []
    unique_attributes = []

    for attributes in column_data:
        attributes_list = attributes.split(';')
        # Exclude "None" while appending other attributes
        all_attributes.extend([attribute.strip() for attribute in attributes_list if attribute.strip() != "None"])

    attribute_counts = pd.Series(all_attributes).value_counts()
    unique_attributes = attribute_counts.index

    return attribute_counts, unique_attributes

factors_count, unique_factors = extract_attributes(test['Which of these factors are important to you when deciding which car to purchase?'])
factors_count, unique_factors

exterior_count, unique_exterior = extract_attributes(test['Which of the following exterior components would you choose to customise (texture, layout, size, etc)? '])
exterior_count, unique_exterior

interior_count, unique_interior = extract_attributes(test['Which of the following interior components would you choose to customise (texture, layout, size, etc)? '])
interior_count, unique_interior

def encode_column(df, column_name, unique_x, category_prefix):
    for attribute in unique_x:
        df[f"{category_prefix}_{attribute.lower().replace(' ', '_')}"] = df[column_name].apply(lambda x: 1 if attribute in x.split(';') else 0)

    df.drop(column_name, axis=1, inplace=True)

encode_column(test, 'Which of these factors are important to you when deciding which car to purchase?', unique_factors, 'purchasefactor')
encode_column(test, 'Which of the following exterior components would you choose to customise (texture, layout, size, etc)? ', unique_exterior, 'exteriorcomp')
encode_column(test, 'Which of the following interior components would you choose to customise (texture, layout, size, etc)? ', unique_interior, 'interiorcomp')

test.head(5)

def encode_categorical_columns(df, column_name):
    unique_categories = df[column_name].unique()

    for category in unique_categories:
        new_column_name = f"{column_name}_{category}"
        df[new_column_name] = df[column_name].apply(lambda x: 1 if x == category else 0)

    df.drop(column_name, axis=1, inplace=True)

columns_for_clustering = [
    'Which age group do you belong to?',
    'What is your gender?',
    'Which category do you currently belong to?',
    'Which of the following best describes you?',
    'How likely are you to opt for customised vehicle if there were no extra charges? ',
    'How much are you willing to spend on car customisation if surcharges are applicable?',
    'Are you interested in designing your own components to personalise your car? ',
    'How much are you willing to pay for the personalised design? ',
    'Do you have any 3D design experience that would help with the design process? (e.g. AutoCAD, SolidWorks, Blender, etc)'
]

for column in columns_for_clustering:
    encode_categorical_columns(test, column)

test = test.drop('Please give us any design ideas to make the car uniquely Singaporean.', axis=1)
test.head(5)

"""## Clustering

### KModes
"""

from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler

cluster_df = cleaned_df.copy()[columns_for_clustering]
cluster_df = cluster_df.astype(str)

!pip install kmodes

import kmodes
from kmodes.kmodes import KModes

# Assuming 'cluster_df' is your DataFrame with categorical columns
# Specify the number of clusters
num_clusters = 3

# Initialize the K-Modes model
kmodes = KModes(n_clusters=num_clusters, init='Huang', n_init=5, verbose=1, random_state=42)

# Fit the model to your data
clusters = kmodes.fit_predict(cluster_df)

# Add the cluster labels to your original DataFrame
cluster_df['Cluster'] = clusters

# Display the resulting clusters
print(cluster_df['Cluster'].value_counts())

import matplotlib.pyplot as plt
import seaborn as sns

plt.figure(figsize=(8, 5))
sns.countplot(x='Cluster', data=cluster_df)
plt.title('Distribution of Data Points in Each Cluster')
plt.show()

cluster_profiles = cluster_df.groupby('Cluster').agg(lambda x: x.value_counts().index[0]).reset_index()
print(cluster_profiles)

for cluster_num in cluster_df['Cluster'].unique():
    cluster_data = cluster_df[cluster_df['Cluster'] == cluster_num]
    print(f"\nCluster {cluster_num} Characteristics:")
    print(cluster_data.describe(include='all'))

"""### MCA + Kmodes"""

!pip install prince
!pip install kmodes

import pandas as pd
from prince import MCA

categorical_columns = [
    'Which age group do you belong to?',
    'What is your gender?',
    'Which category do you currently belong to?',
    'Which of the following best describes you?',
    'How likely are you to opt for customised vehicle if there were no extra charges? ',
    'How much are you willing to spend on car customisation if surcharges are applicable?',
    'Are you interested in designing your own components to personalise your car? ',
    'How much are you willing to pay for the personalised design? ',
    'Do you have any 3D design experience that would help with the design process? (e.g. AutoCAD, SolidWorks, Blender, etc)'
]

mca_df = cleaned_df.copy()[columns_for_clustering]
mca_df = mca_df.astype(str)

# Initialize the MCA model
mca = MCA(n_components=2)

# Fit and transform the MCA model on the categorical data
df_mca = mca.fit_transform(mca_df)
loadings = mca.row_coordinates(mca_df)

# Add the MCA components to the original DataFrame
mca_df[['MCA Component 1', 'MCA Component 2']] = df_mca

# Display the resulting DataFrame
print(mca_df.head(10))

# Access the proportion of total inertia explained by each component
eigenvalues = mca.eigenvalues_

# Calculate the explained inertia from eigenvalues
explained_inertia = eigenvalues / eigenvalues.sum()

# Display the proportion of total inertia explained by each component
print("Explained Inertia:")
print(explained_inertia)

print(mca.eigenvalues_)

print(mca.column_contributions_)

from kmodes.kmodes import KModes

# Specify the MCA components and other numerical features
numerical_columns = ['MCA Component 1', 'MCA Component 2']
features_for_clustering = categorical_columns + numerical_columns

# Initialize the K-Modes model
num_clusters = 3
kmodes = KModes(n_clusters=num_clusters, init='Huang', n_init=5, verbose=1, random_state=42)

# Fit the model to your data
clusters = kmodes.fit_predict(mca_df[features_for_clustering])

# Add the cluster labels to your original DataFrame
mca_df['Cluster'] = clusters

# Display the resulting clusters
print(mca_df['Cluster'].value_counts())

import matplotlib.pyplot as plt
import seaborn as sns

plt.figure(figsize=(8, 5))
sns.countplot(x='Cluster', data=mca_df)
plt.title('Distribution of Data Points in Each Cluster')
plt.show()

plt.figure(figsize=(10, 6))
sns.scatterplot(x='MCA Component 1', y='MCA Component 2', hue='Cluster', data=mca_df, palette='viridis')
plt.title('MCA Component Scatter Plot by Cluster')
plt.show()

cluster_profiles = mca_df.groupby('Cluster').agg(lambda x: x.value_counts().index[0]).reset_index()
print(cluster_profiles)

for cluster_num in mca_df['Cluster'].unique():
    cluster_data = mca_df[mca_df['Cluster'] == cluster_num]
    print(f"\nCluster {cluster_num} Characteristics:")
    print(cluster_data.describe(include='all'))

# Calculate centroids manually
centroids = mca_df.groupby('Cluster')[numerical_columns].mean().reset_index()

# Scatter plot of MCA components with cluster centroids
plt.figure(figsize=(10, 6))
sns.scatterplot(x='MCA Component 1', y='MCA Component 2', hue='Cluster', data=mca_df, palette='viridis')
sns.scatterplot(x=centroids['MCA Component 1'], y=centroids['MCA Component 2'], marker='X', s=200, color='red')
plt.title('MCA Component Scatter Plot with Cluster Centroids')
plt.show()

"""## Correlation"""

column_names = test.columns
print(column_names)

# Columns for correlation overall
columns_for_correlation_overall = ['exteriorcomp_wheels','exteriorcomp_headlights','exteriorcomp_grilles','exteriorcomp_bumpers' ,'exteriorcomp_side_mirrors','interiorcomp_dashboard','interiorcomp_steering_wheel','interiorcomp_centre_compartment','interiorcomp_door_handles']

# Compute the correlation matrix
correlation_matrix_overall =test[columns_for_correlation_overall].corr()

# Generate a heatmap
plt.figure(figsize=(10, 8))
sns.heatmap(correlation_matrix_overall, annot=True, cmap='coolwarm', fmt=".2f", linewidths=.5)
plt.title('Correlation Matrix of Numerical Variables')
plt.show()

"""From this heatmap, we can found that a person like to customize side mirrors have high chance want to customize headlights and door handles. Besides, those who customized steering wheel also customized wheels as well.

## Association Rule Mining

Market Basket Analysis: if A is bought, then B is likely to bought as well
"""

print(test.columns)

"""### Exterior Components"""

from mlxtend.frequent_patterns import apriori
from mlxtend.frequent_patterns import association_rules

ext=[  'exteriorcomp_wheels',
       'exteriorcomp_headlights', 'exteriorcomp_grilles',
       'exteriorcomp_bumpers', 'exteriorcomp_side_mirrors',
       'exteriorcomp_brakes', 'exteriorcomp_doors',
       'exteriorcomp_add_body_kit_and_change_the_exhaust_and_tune_the_engine']

components=test[ext].astype('bool')

frequent_itemsets = apriori(components, min_support=0.0001, use_colnames=True)

rules = association_rules(frequent_itemsets, metric="lift")

rules.sort_values(["support", "confidence","lift"],axis = 0, ascending = False).head(10)

"""### Interior Components"""

from mlxtend.frequent_patterns import apriori
from mlxtend.frequent_patterns import association_rules

interior = [       'interiorcomp_dashboard', 'interiorcomp_steering_wheel',
       'interiorcomp_centre_compartment', 'interiorcomp_door_handles',
       'interiorcomp_sun_blocker_for_front_passengers',
       'interiorcomp_air_vent']

components=test[interior].astype('bool')

frequent_itemsets = apriori(components, min_support=0.0001, use_colnames=True)

rules = association_rules(frequent_itemsets, metric="lift")

rules.sort_values(["support", "confidence","lift"],axis = 0, ascending = False).head(10)

"""### Exterior and Interior Components"""

from mlxtend.frequent_patterns import apriori
from mlxtend.frequent_patterns import association_rules

both = [       'interiorcomp_dashboard', 'interiorcomp_steering_wheel',
       'interiorcomp_centre_compartment', 'interiorcomp_door_handles',
       'interiorcomp_sun_blocker_for_front_passengers',
       'interiorcomp_air_vent',

       'exteriorcomp_wheels',
       'exteriorcomp_headlights', 'exteriorcomp_grilles',
       'exteriorcomp_bumpers', 'exteriorcomp_side_mirrors',
       'exteriorcomp_brakes', 'exteriorcomp_doors',
       'exteriorcomp_add_body_kit_and_change_the_exhaust_and_tune_the_engine']

components=test[both].astype('bool')

frequent_itemsets = apriori(components, min_support=0.0001, use_colnames=True)

rules = association_rules(frequent_itemsets, metric="lift")

rules.sort_values(["support", "confidence","lift"],axis = 0, ascending = False).head(10)

"""### Purchasing Factors"""

from mlxtend.frequent_patterns import apriori
from mlxtend.frequent_patterns import association_rules

fact = ['purchasefactor_price', 'purchasefactor_functionality',
       'purchasefactor_brand_name', 'purchasefactor_technological_features',
       'purchasefactor_aesthetics', 'purchasefactor_size',
       'purchasefactor_sustainability/environment_considerations',
       'purchasefactor_customisable_options']

components=test[fact].astype('bool')

frequent_itemsets = apriori(components, min_support=0.0001, use_colnames=True)

rules = association_rules(frequent_itemsets, metric="lift")

rules.sort_values(["support", "confidence","lift"],axis = 0, ascending = False).head(10)

from mlxtend.frequent_patterns import apriori
from mlxtend.frequent_patterns import association_rules

fact = [ 'Are you interested in designing your own components to personalise your car? _Only with professional help',
       'Are you interested in designing your own components to personalise your car? _Yes',
       'Are you interested in designing your own components to personalise your car? _No',
       'How much are you willing to pay for the personalised design? _500-1000',
       'How much are you willing to pay for the personalised design? _under 500',
       'How much are you willing to pay for the personalised design? _over 1000',
       'How much are you willing to pay for the personalised design? _0',
       'Do you have any 3D design experience that would help with the design process? (e.g. AutoCAD, SolidWorks, Blender, etc)_No, I would need a designer to model my sketch',
       'Do you have any 3D design experience that would help with the design process? (e.g. AutoCAD, SolidWorks, Blender, etc)_Yes, I can design on my own']

components=test[fact].astype('bool')

frequent_itemsets = apriori(components, min_support=0.0001, use_colnames=True)

rules = association_rules(frequent_itemsets, metric="lift")

rules.sort_values(["support", "confidence","lift"],axis = 0, ascending = False).head(10)